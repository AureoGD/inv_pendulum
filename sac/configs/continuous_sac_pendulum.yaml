# Environment settings
env_id: "Pendulum-v1"

# Trainer settings
agent_class_name: "SacCasAgent" # To be used by run_experiment.py to import the class
agent_module_name: "sac_cas.sac_cas_agent" # Path to the agent module

training_total_timesteps: 50000   # Or a bit more to see clear learning, e.g., 100k
max_steps_per_episode: 200      
log_interval_timesteps: 2000    
eval_frequency_timesteps: 5000  
n_eval_episodes: 5              
save_freq_episodes: 50         
log_root: "dsac/data/runs/sac"     
model_root: "dsac/data/models/sac"   

# Agent Hyperparameters (passed to Trainer, then to SacCasAgent constructor)
# obs_shapes_or_space will be set by run_experiment.py from env.observation_space.shape
# action_spec_for_buffer will be set by run_experiment.py from env.action_space
use_encoder: False              # Crucial for using simple MLP/Identity encoder
encoder_mlp_hidden_dims: [] # Optional: for the SimpleMLPEncoder if obs are flat
                                 # Or set to [] for Identity-like behavior if MLP is in main net body
gamma: 0.99                     
tau: 0.005                      
alpha_init: "auto"                   
critic_lr: 0.0003               
actor_lr: 0.0003                
replay_buffer_size: 100000     # Can be smaller for Pendulum
batch_size: 256                 
learning_starts: 1000           # Start learning after 1000 steps
gradient_steps: 1               
policy_delay: 2                 
max_grad_norm: 1.0              # Example, or null
reward_scale: 1.0               
